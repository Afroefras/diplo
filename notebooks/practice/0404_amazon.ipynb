{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predict pollution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/Users/efraflores/Desktop/EF/Diplo/data/04/amazon')\n",
    "TRAIN_NAME = 'amazon_train.csv'\n",
    "VAL_NAME = 'amazon_test.csv'\n",
    "MAX_WORDS = 10000\n",
    "MAX_SEQ = 64\n",
    "EMBEDDING_DIM = 132\n",
    "EPOCHS = 22\n",
    "BATCH_SIZE = 2000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(BASE_DIR.joinpath(TRAIN_NAME)).set_index('review_id')\n",
    "df.sample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Uncomment the following lines if it's the first time you run this packages\n",
    "'''\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "import re\n",
    "import unicodedata\n",
    "from emoji import demojize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text, language='english', pattern=\"[^a-zA-Z\\s]\", add_stopw=[],\n",
    "                lower=False, lemma=False, rem_stopw=False, unique=False, emoji=False):\n",
    "    if emoji: text = demojize(text)\n",
    "    cleaned_text = unicodedata.normalize('NFD',str(text).replace('\\n',' \\n ')).encode('ascii', 'ignore')\n",
    "    cleaned_text = re.sub(pattern,' ',cleaned_text.decode('utf-8'),flags=re.UNICODE)\n",
    "    cleaned_text = [(lem.lemmatize(word,pos='v') if lemma else word) for word in \n",
    "                    (cleaned_text.lower().split() if lower else cleaned_text.split())]\n",
    "    if rem_stopw: cleaned_text = [word for word in cleaned_text if word not in \n",
    "                                  stopwords.words(language)+add_stopw]\n",
    "    return ' '.join((set(cleaned_text) if unique else cleaned_text))\n",
    "\n",
    "#Ex\n",
    "ex = \"I am going to run!!! I ran while I was running??? ...\"\n",
    "print('\\nOriginal:\\t\\t',ex)\n",
    "print('Basic cleaning:\\t\\t',clean_text(ex))\n",
    "print('Changing the pattern:\\t',clean_text(ex,pattern=\"[^a-zA-Z!\\.]\"))\n",
    "print('Without stopwords:\\t',clean_text(ex,rem_stopw=True))\n",
    "print('Lower and lemma:\\t',clean_text(ex,lower=True,lemma=True))\n",
    "print('Super cleaning:\\t\\t',clean_text(ex,add_stopw=['go'],lower=True,rem_stopw=True,lemma=True,unique=True))\n",
    "print(\"\\nIt actually corrects the weird accents, example\\n\\tFROM:\\t Th√à √âfr√¢√ØsM√£'s?...\\n\\tTO:\\t\",clean_text(\"Th√à √âfr√¢√ØsMa's?...\",lower=True))\n",
    "print(\"\\nAnd now, it can translate emojis!!! üòç\",clean_text('üòç', emoji=True))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def outliers(data,cols):\n",
    "    df = data.copy()\n",
    "    outlier = IsolationForest(contamination=.04,n_jobs=-1)\n",
    "    df['outlier'] = outlier.fit_predict(df[cols])\n",
    "    df = df[df['outlier']!=-1].drop(columns = 'outlier')\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Confussion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def cm_keras(X, y, fit_model, label_encod=None, scale_to=1):\n",
    "    y_real = [np.argmax(x) for x in y]\n",
    "    y_predicted = [np.argmax(x).reshape(-1,)[0] for x in fit_model.predict(X)]\n",
    "\n",
    "    if label_encod == None: pass\n",
    "    else: \n",
    "        y_real = label_encod.inverse_transform(y_real)\n",
    "        y_predicted = label_encod.inverse_transform(y_predicted)\n",
    "\n",
    "    cm = pd.DataFrame(confusion_matrix(y_true=y_real, y_pred=y_predicted),\n",
    "                    index=label_encod.classes_ if label_encod!=None else label_encod,\n",
    "                    columns=label_encod.classes_ if label_encod!=None else label_encod).replace({0:np.nan}).T\n",
    "\n",
    "    print('Accuracy de {:.2%}'.format(np.asarray(cm).trace()/len(y_test)))\n",
    "    size = int(len(np.unique(y_real))/2)*scale_to\n",
    "    fig, ax = plt.subplots(figsize=(size,size)) \n",
    "    sns.heatmap(pd.DataFrame([cm[col]/cm[col].sum() for col in cm.columns]), \n",
    "                annot = True,\n",
    "                fmt = '.0%',\n",
    "                cmap = 'Blues',\n",
    "                linewidths = 0.5, \n",
    "                ax = ax)\n",
    "    plt.show()\n",
    "    return cm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def full_pipeline(data):\n",
    "    df = data.copy()\n",
    "    df.fillna({'title':'empty title', 'body':'empty body'}, inplace=True)\n",
    "    df['text'] = df['title'].astype(str) + ' ' + df['body'].astype(str)\n",
    "    df['clean_text'] = df['text'].map(lambda x: clean_text(x, lower=True, rem_stopw=True, lemma=True, emoji=True))\n",
    "    df.fillna({'clean_text':'empty text'}, inplace=True)\n",
    "    X = pad_sequences(tokenizer.texts_to_sequences(df['clean_text'].values), maxlen=MAX_SEQ)\n",
    "    predictions = model.predict(X)\n",
    "    return [le.inverse_transform([np.argmax(x)])[0] for x in predictions]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.fillna({'title':'empty title', 'body':'empty body'}, inplace=True)\n",
    "df['text'] = df['title'].astype(str) + ' ' + df['body'].astype(str)\n",
    "df[['text']].sample(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['len'] = df['text'].str.split().str.len()\n",
    "df['len'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = outliers(df, ['len'])\n",
    "df['len'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# df['clean_text'] = df['text'].map(lambda x: clean_text(x, lower=True, rem_stopw=True, lemma=True, emoji=True))\n",
    "# df.to_csv(BASE_DIR.joinpath(f'clean_{TRAIN_NAME}'))\n",
    "# df[['text', 'clean_text']].sample(4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv(BASE_DIR.joinpath(f'clean_{TRAIN_NAME}')).set_index('review_id')\n",
    "df.fillna({'clean_text':'empty text'}, inplace=True)\n",
    "df.sample()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizer and Padding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['clean_text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens')\n",
    "X = pad_sequences(tokenizer.texts_to_sequences(df['clean_text'].values), maxlen=MAX_SEQ)\n",
    "print(X[22])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Target encoder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = to_categorical(le.fit_transform(df['rating']))\n",
    "print(le.classes_,'\\n',y[22])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train test split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22, train_size=0.77, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arquitecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, activation=\"tanh\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(len(le.classes_), activation='softmax'))\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Callbacks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "checkpoint = ModelCheckpoint(BASE_DIR.joinpath('models','amazon_model_{val_accuracy:.3f}.h5'),\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             monitor='val_accuracy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cufflinks\n",
    "cufflinks.go_offline()\n",
    "\n",
    "metrics = pd.DataFrame(data = zip(training_history.history[\"loss\"], training_history.history[\"val_loss\"], training_history.history[\"accuracy\"], training_history.history[\"val_accuracy\"]), columns=[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"])\n",
    "metrics.iplot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confussion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conf_matrix = cm_keras(X_test, y_test, model, label_encod=le, scale_to=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val = pd.read_csv(BASE_DIR.joinpath(VAL_NAME)).set_index('review_id')\n",
    "val['y_hat'] = full_pipeline(val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val[['y_hat']].to_csv(BASE_DIR.joinpath(f'predict_{VAL_NAME}'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "9de603709bc7399ed455492376c092aaf43a9de23cd182bce02e28baeb746ca5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}