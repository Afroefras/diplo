{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d737e6",
   "metadata": {},
   "source": [
    "# Tweet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7596622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:52:20.685896Z",
     "start_time": "2021-06-06T00:52:20.681869Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/efraflores/Desktop/hub/diplo/venv/lib/python3.9/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67714952",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8184ab1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:52:22.690232Z",
     "start_time": "2021-06-06T00:52:21.058500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from pandas import read_csv\n",
    "\n",
    "'''!pip install bs4'''\n",
    "from re import sub, UNICODE, findall\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize\n",
    "\n",
    "'''\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "from numpy import append,zeros,array\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b48ca",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70f438",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcb996e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:52:22.713289Z",
     "start_time": "2021-06-06T00:52:22.710426Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/efraflores/Desktop/EF/Diplo/data/05'\n",
    "FILE_NAME = '0505_tuit.csv'\n",
    "TOKENIZER = 'tuit_tokenizer.pickle'\n",
    "MODEL_SUP = 'tuit_model_supervised.pickle'\n",
    "MODEL_UNSUP = 'tuit_model_unsupervised.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea08b4b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997ae4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:52:28.897576Z",
     "start_time": "2021-06-06T00:52:28.154993Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_date_created</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>language</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>993077604482736131</th>\n",
       "      <td>2018-05-06T10:38:46</td>\n",
       "      <td>La #PremierLeague ha anunciado a los seis cand...</td>\n",
       "      <td>es</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>{\"Neutral\":0.8101093769073486328125,\"Negative\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_date_created  \\\n",
       "tweet_id                                  \n",
       "993077604482736131  2018-05-06T10:38:46   \n",
       "\n",
       "                                                           tweet_text  \\\n",
       "tweet_id                                                                \n",
       "993077604482736131  La #PremierLeague ha anunciado a los seis cand...   \n",
       "\n",
       "                   language sentiment  \\\n",
       "tweet_id                                \n",
       "993077604482736131       es   NEUTRAL   \n",
       "\n",
       "                                                      sentiment_score  \n",
       "tweet_id                                                               \n",
       "993077604482736131  {\"Neutral\":0.8101093769073486328125,\"Negative\"...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(path.join(BASE_DIR,FILE_NAME),encoding='latin').set_index('tweet_id')\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5fcee",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd25d5a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clean tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859df28e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:54:21.116080Z",
     "start_time": "2021-06-06T00:54:21.110027Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "  # Obtenemos el texto:\n",
    "  text = BeautifulSoup(normalize('NFD', text).encode('ascii', 'ignore'), \"lxml\").get_text()\n",
    "  # Eliminación de las menciones en los tweets. Se excluyen los @ juntos con todos los caracteres que le siguen (De la A a la Z, en minúsculas y mayusculas, y números del 0 al 9)\n",
    "  #text = sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
    "  # Eliminación de los links que inicien con https o http. Inicamos que la s es opcional (s?).\n",
    "  text = sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
    "  # Eliminación de los links que inicien con www.\n",
    "  text = sub(r\"www.[A-Za-z0-9./]+\", ' ', text)\n",
    "  # Eliminación de todos los catacteres menos las letras y signos de puntuación. \n",
    "  text = sub(r\"[^a-zA-Z.!?']\", ' ', text)\n",
    "  # Eliminamos espacios en blanco dobles.\n",
    "  text = sub(r\" +\", ' ', text)\n",
    "  return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904f640",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c54e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:54:22.044726Z",
     "start_time": "2021-06-06T00:54:22.038195Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text,\n",
    "               language='english',pattern=\"[^a-zA-Z'\\s]\",\n",
    "               lower=False,lemma=False,rem_stopw=False,unique=False,\n",
    "               add_stopw=[]):\n",
    "    #It clean and can remove stopwords or even lemmatize words if specified in params\n",
    "    cleaned_text = normalize('NFD',str(text).replace('\\n',' \\n ')).encode('ascii', 'ignore')\n",
    "    cleaned_text = sub(pattern,' ',cleaned_text.decode('utf-8'),flags=UNICODE)\n",
    "    cleaned_text = [(lem.lemmatize(word,pos='v') if lemma else word) for word in \n",
    "                    (cleaned_text.lower().split() if lower else cleaned_text.split())]\n",
    "    if rem_stopw: cleaned_text = [word for word in cleaned_text if word not in \n",
    "                                  stopwords.words(language)+add_stopw]\n",
    "    return ' '.join((set(cleaned_text) if unique else cleaned_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fad01",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02da0756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:54:23.595385Z",
     "start_time": "2021-06-06T00:54:23.590814Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def manual_pad(array,n=170):\n",
    "    return append(array,zeros(max(0,n-len(array))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b092f1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Read objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2e8908",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:54:24.521402Z",
     "start_time": "2021-06-06T00:54:24.515175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def read_obj():\n",
    "    with open(path.join(BASE_DIR,TOKENIZER), 'rb') as f: \n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    with open(path.join(BASE_DIR,MODEL_SUP), 'rb') as f: \n",
    "        model_sup = pickle.load(f)\n",
    "\n",
    "    with open(path.join(BASE_DIR,MODEL_UNSUP), 'rb') as f: \n",
    "        model_unsup = pickle.load(f)\n",
    "    return tokenizer,model_sup,model_unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43128b58",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a753a537",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:58:21.359291Z",
     "start_time": "2021-06-06T00:58:21.347448Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def full_pipeline(text,fitted_tokenizer,supervised_model,unsupervised_model,\n",
    "                  text_col='tweet_text'):\n",
    "    original_text = text\n",
    "    tokenizer = fitted_tokenizer\n",
    "    model_sup = supervised_model\n",
    "    model_unsup = unsupervised_model\n",
    "    \n",
    "    text = clean_tweet(text)\n",
    "    text = ' '.join(text.split()[:170])\n",
    "    text = tokenizer.encode(text,add_special_tokens=True)\n",
    "    text = manual_pad(text)\n",
    "    output = {'time_stamp':f'{datetime.now().strftime(\"%d/%m/%YT%H:%M\")}',\n",
    "              'team_name':'Untitled'}\n",
    "    aux_dict = {}\n",
    "    for x,y in zip(model_sup.classes_,model_sup.predict_proba([text])[0]):\n",
    "        aux_dict[x] = round(y,3)\n",
    "        \n",
    "    rename_dict = {'proba_positive': 'POSITIVE', \n",
    "                   'proba_negative': 'NEGATIVE', \n",
    "                   'proba_neutral': 'NEUTRAL', \n",
    "                   'proba_mixed': 'MIXED'}\n",
    "    \n",
    "    for x,y in rename_dict.items():\n",
    "        output[x] = aux_dict[rename_dict[x]]\n",
    "    \n",
    "    output['class'] = model_sup.predict([text])[0]\n",
    "    \n",
    "    text = original_text\n",
    "    clean = clean_text(text,language='spanish',lower=True,pattern=\"[^a-zA-Z\\s]\")\n",
    "    min_text = clean_text(text,language='spanish',lower=True,pattern=\"[^a-zA-Z\\s]\",\n",
    "                          unique=True)\n",
    "\n",
    "    var_unsup = [aux_dict['NEUTRAL'],aux_dict['NEGATIVE'],aux_dict['POSITIVE'],aux_dict['MIXED'],\n",
    "                 len(text),len(min_text)/(len(clean)+1e-10),len(findall('@',text)),\n",
    "                 len(findall('#',text)),len(findall('[A-Z]',text)),\n",
    "                 len(text)/len(text.split())]\n",
    "    \n",
    "    cluster_dict = {1:'Amorosos',2:'Adictos al #',3:'Faroles',4:'Haters'}\n",
    "    \n",
    "    output['cluster'] = cluster_dict[model_unsup.predict(array((var_unsup,)))[0]]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecda3b",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993aefc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-06T00:58:22.288534Z",
     "start_time": "2021-06-06T00:58:22.254340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_stamp': '05/06/2021T19:58',\n",
       " 'team_name': 'Untitled',\n",
       " 'proba_positive': 0.091,\n",
       " 'proba_negative': 0.099,\n",
       " 'proba_neutral': 0.803,\n",
       " 'proba_mixed': 0.007,\n",
       " 'class': 'NEUTRAL',\n",
       " 'cluster': 'Faroles'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,model_sup,model_unsup = read_obj()\n",
    "text = df.sample()['tweet_text'].values[0]\n",
    "full_pipeline(text,tokenizer,model_sup,model_unsup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
